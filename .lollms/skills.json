[
  {
    "id": "lollms-client-lib",
    "name": "LoLLMs Client Library",
    "description": "Comprehensive documentation for the lollms_client Python library.",
    "content": "# LoLLMs Client Library\n\n[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![PyPI version](https://badge.fury.io/py/lollms_client.svg)](https://badge.fury.io/py/lollms_client)\n[![Python Versions](https://img.shields.io/pypi/pyversions/lollms_client.svg)](https://pypi.org/project/lollms-client/)\n[![Downloads](https://static.pepy.tech/personalized-badge/lollms-client?period=total&units=international_system&left_color=grey&right_color=green&left_text=Downloads)](https://pepy.tech/project/lollms-client)\n\n**`lollms_client`** is a powerful and flexible Python library designed to simplify interactions with the **LoLLMs (Lord of Large Language Models)** ecosystem and various other Large Language Model (LLM) backends. It provides a unified API for text generation, multimodal operations (text-to-image, text-to-speech, etc.), and robust function calling through the Model Context Protocol (MCP).\n\nWhether you're connecting to a remote LoLLMs server, an Ollama instance, the OpenAI API, or running models locally using GGUF (via `llama-cpp-python` or a managed `llama.cpp` server), Hugging Face Transformers, or vLLM, `lollms-client` offers a consistent and developer-friendly experience.\n\n## Key Features\n\n*   ðŸ”Œ **Versatile Binding System:** Seamlessly switch between different LLM backends (LoLLMs, Ollama, OpenAI, Llama.cpp, Transformers, vLLM, OpenLLM, Gemini, Claude, Groq, OpenRouter, Hugging Face Inference API) using a unified `llm_binding_config` dictionary for all parameters.\n*   ðŸ—£ï¸ **Comprehensive Multimodal Support:** Interact with models capable of processing images and generate various outputs like speech (TTS), video (TTV), and music (TTM).\n*   ðŸŽ¨ **Advanced Image Generation and Editing:** A new `diffusers` binding provides powerful text-to-image capabilities. It supports a wide range of models from Hugging Face and Civitai, including specialized models like `Qwen-Image-Edit` for single-image editing and the cutting-edge `Qwen-Image-Edit-2509` for **multi-image fusion, pose transfer, and character swapping**.\n*   ðŸ–¼ï¸ **Selective Image Activation:** Control which images in a message are active and sent to the model, allowing for fine-grained multimodal context management without deleting the original data.\n*   ðŸ¤– **Agentic Workflows with MCP:** Empower LLMs to act as sophisticated agents, breaking down complex tasks, selecting and executing external tools (e.g., internet search, code interpreter, file I/O, image generation) through the Model Context Protocol (MCP) using a robust \"observe-think-act\" loop.\n*   ðŸŽ­ **Personalities as Agents:** Personalities can now define their own set of required tools (MCPs) and have access to static or dynamic knowledge bases (`data_source`), turning them into self-contained, ready-to-use agents.\n*   ðŸš€ **Streaming & Callbacks:** Efficiently handle real-time text generation with customizable callback functions across all generation methods, including during agentic (MCP) interactions.\n*   ðŸ“‘ **Long Context Processing:** The `long_context_processing` method (formerly `sequential_summarize`) intelligently chunks and synthesizes texts that exceed the model's context window, suitable for summarization or deep analysis.\n*   ðŸ“ **Advanced Structured Content Generation:** Reliably generate structured JSON output from natural language prompts using the `generate_structured_content` helper method, enforcing a specific schema.\n*   ðŸ’¬ **Advanced Discussion Management:** Robustly manage conversation histories with `LollmsDiscussion`, featuring branching, context exporting, and automatic pruning.\n*   ðŸ§  **Persistent Memory & Data Zones:** `LollmsDiscussion` now supports multiple, distinct data zones (`user_data_zone`, `discussion_data_zone`, `personality_data_zone`) and a long-term `memory` field. This allows for sophisticated context layering and state management, enabling agents to learn and remember over time.\n*   âœï¸ **Structured Memorization:** The `memorize()` method analyzes a conversation to extract its essence (e.g., a problem and its solution), creating a structured \"memory\" with a title and content. These memories are stored and can be explicitly loaded into the AI's context, providing a more robust and manageable long-term memory system.\n*   ðŸ“Š **Detailed Context Analysis:** The `get_context_status()` method provides a rich, detailed breakdown of the prompt context, showing the content and token count for each individual component (system prompt, data zones, message history).\n*   âš™ï¸ **Standardized Configuration Management:** A unified dictionary-based system (`llm_binding_config`) to configure any binding in a consistent manner.\n*   ðŸ§© **Extensible:** Designed to easily incorporate new LLM backends and modality services, including custom MCP toolsets.\n*   ðŸ“ **High-Level Operations:** Includes convenience methods for complex tasks like sequential summarization and deep text analysis directly within `LollmsClient`.\n\n## Installation\n\nYou can install `lollms_client` directly from PyPI:\n\n```bash\npip install lollms-client\n```\n\n## Core Generation Methods\n\n### Basic Text Generation (`generate_text`)\n\n```python\nfrom lollms_client import LollmsClient, MSG_TYPE\nfrom ascii_colors import ASCIIColors\nimport os\n\n# Callback for streaming output\ndef simple_streaming_callback(chunk: str, msg_type: MSG_TYPE, params=None, metadata=None) -> bool:\n    if msg_type == MSG_TYPE.MSG_TYPE_CHUNK:\n        print(chunk, end=\"\", flush=True)\n    return True # True to continue streaming\n\ntry:\n    # Initialize client to connect to a LoLLMs server.\n    lc = LollmsClient(\n        llm_binding_name=\"lollms\", \n        llm_binding_config={\n            \"host_address\": \"http://localhost:9642\",\n        }\n    )\n\n    prompt = \"Tell me a fun fact about space.\"\n    response_text = lc.generate_text(\n        prompt,\n        n_predict=100,\n        stream=True,\n        streaming_callback=simple_streaming_callback\n    )\nexcept Exception as e:\n    ASCIIColors.error(f\"An unexpected error occurred: {e}\")\n```\n\n### Generating from Message Lists (`generate_from_messages`)\n\n```python\nfrom lollms_client import LollmsClient, MSG_TYPE\nimport os\n\ntry:\n    lc = LollmsClient(\n        llm_binding_name=\"ollama\", \n        llm_binding_config={\n            \"model_name\": \"llama3\",\n            \"host_address\": \"http://localhost:11434\"\n        }\n    )\n\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n\n    response_text = lc.generate_from_messages(\n        messages=messages,\n        n_predict=200,\n        stream=True\n    )\nexcept Exception as e:\n    print(f\"Error: {e}\")\n```\n\n## Advanced Discussion Management\n\n### Basic Chat with `LollmsDiscussion`\n\n```python\nfrom lollms_client import LollmsClient, LollmsDiscussion, MSG_TYPE, LollmsDataManager\nfrom pathlib import Path\nimport tempfile\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    db_path = Path(tmpdir) / \"discussion_db.sqlite\"\n    db_manager = LollmsDataManager(f\"sqlite:///{db_path}\")\n    \n    lc = LollmsClient(llm_binding_name=\"ollama\", llm_binding_config={\"model_name\": \"llama3\"})\n\n    discussion = LollmsDiscussion.create_new(\n        lollms_client=lc,\n        db_manager=db_manager,\n        id=\"basic_chat_example\",\n        autosave=True\n    )\n    \n    response = discussion.chat(user_message=\"Hello, how are you?\")\n    print(response['ai_message'].content)\n```\n\n### Managing Multimodal Context: Activating and Deactivating Images\n\nWhen working with multimodal models, you can control which images in a message are active.\n\n```python\n# ... setup discussion ...\ndiscussion.add_message(\n    sender=\"user\", \n    content=\"What is in the image?\", \n    images=[img1_b64, img2_b64]\n)\nuser_message = discussion.get_messages()[-1]\n\n# Deactivate irrelevant images\nuser_message.toggle_image_activation(index=0, active=False)\ndiscussion.commit()\n```\n\n### Agentic Workflows\n\nExample of a Python Coder Agent using `LollmsPersonality`:\n\n```python\nfrom lollms_client import LollmsClient, LollmsPersonality, LollmsDiscussion\n\ncoder_personality = LollmsPersonality(\n    name=\"Python Coder Agent\",\n    author=\"lollms-client\",\n    category=\"Coding\",\n    description=\"An agent that writes and executes Python code.\",\n    system_prompt=\"You are an expert Python programmer. Use the python_code_interpreter tool.\",\n    active_mcps=[\"python_code_interpreter\"]\n)\n\nlc = LollmsClient(\n    llm_binding_name=\"ollama\",          \n    llm_binding_config={\"model_name\": \"codellama\"},\n    mcp_binding_name=\"local_mcp\" \n)\n\n# ... setup discussion ...\nresponse = discussion.chat(\n    user_message=\"Write a Python function to sum two numbers.\",\n    personality=coder_personality,\n    max_llm_iterations=5\n)\n```\n\n## Using LoLLMs Client with Different Bindings\n\nYou can configure different backends using `llm_binding_config`:\n\n**LoLLMs Server:**\n```python\nconfig = { \"host_address\": \"http://localhost:9642\" }\n```\n\n**Ollama:**\n```python\nconfig = { \"model_name\": \"llama3\", \"host_address\": \"http://localhost:11434\" }\n```\n\n**OpenAI:**\n```python\nconfig = { \"model_name\": \"gpt-4o\", \"service_key\": \"sk-...\" }\n```\n\n**Anthropic Claude:**\n```python\nconfig = { \"model_name\": \"claude-3-5-sonnet-20240620\", \"service_key\": \"sk-ant-...\" }\n```\n\n**Diffusers (Local Image Gen):**\n```python\nlc = LollmsClient(\n    tti_binding_name=\"diffusers\",\n    tti_binding_config={\n        \"model_name\": \"runwayml/stable-diffusion-v1-5\"\n    }\n)\nimage_bytes = lc.generate_image(\"Astronaut on Mars\")\n```\n",
    "language": "markdown",
    "timestamp": 1765266670523
  }
]